'''

交叉熵损失

MSE 主要用于预测  线性回归
Cross Entropy 主要用于多分类 logistic 回归  神经网络
Hinge Loss 主要用于最大化间隔距离  SVM支持向量机



'''

import matplotlib.pyplot as plt
import numpy as np
y = np.linspace(0, 1, 100)
delta = 1e-7
l = -np.log(y + delta)
plt.title("loss = -log(y)")
plt.plot(y, l)
plt.ylim(0, 5)
plt.xlim(0, 1.0)
plt.show()

'''
交叉熵损失来说，需要一个非线性的映射

交叉熵损失函数将其进一步作为输入输进sigmoid的函数中
使用交叉熵损失函数的时候，真实值只有两种情况，一种是0，一种是1。
而交叉熵损失中的模型输出在0到1之间。我们使用log函数来表示真实值与模型输出之间的关系。而真实值有两个

交叉熵损失函数从模型输出到损失函数，使用了对数函数，其特点是真实值要么是0要么是1。
损失函数可以让越接近真实值的损失值接近于0，远离真实值的损失值趋向于无穷大。其常用于分类问题中


   首先，可以证明该函数是大于0的
   其次，当y=0时（即真实值为0）若a=0(计算值为0)，c = 0*ln0 + 1*ln1  等于0(前项等于0，后项也等于0)。
   当y=1时，若a=0, c = ln0 +  0*ln1等于无穷大。
即：计算值与真实值差距越大，损失函数越大。反之亦然。
所以cross-entropy满足我们对损失函数的定义。所以他是个合格的损失函数



'''